{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Scheduler System\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design: \n",
    "- Peer Review: [Mark()](https://github.com/obov), [Taylor(Jihyun Kim)](https://github.com/Taylor0819)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Multi-Agent Scheduler System is an advanced automation solution designed to process, schedule, and deliver information based on user queries. The system employs multiple specialized agents working in coordination, backed by a vector database for efficient information storage and retrieval. This system particularly excels at handling complex queries that involve timing specifications and information retrieval tasks.\n",
    "\n",
    "For example, when a user requests \"Find and recommend RAG-related papers at 7 AM,\" the system coordinates multiple agents to analyze the query, schedule the task, gather information, and deliver results at the specified time.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "1) Query Analysis and Task Distribution\n",
    "- Specialized QueryAnalysisAgent breaks down complex user requests\n",
    "- Extracts timing information, search parameters, and user intent\n",
    "- Distributes tasks to appropriate specialized agents\n",
    "\n",
    "2) Information Management\n",
    "- Integrates with SerpAPI for real-time web information retrieval\n",
    "- Utilizes ChromaDB as a vector database for efficient information storage\n",
    "- Implements RAG (Retrieval-Augmented Generation) for enhanced response quality\n",
    "\n",
    "3) Scheduling and Delivery\n",
    "- Manages timing of information delivery through dedicated SchedulerAgent\n",
    "- Handles multiple concurrent requests and scheduling conflicts\n",
    "- Provides status updates and delivery confirmations\n",
    "\n",
    "**Key Components**:\n",
    "\n",
    "- **Specialized Agents**: \n",
    "  - SchedulerAgent: Time management and delivery coordination\n",
    "  - QueryAnalysisAgent: User intent analysis and task distribution\n",
    "  - SearchAgent: SerpAPI integration and information gathering\n",
    "  - RAGAgent: Information processing and summarization\n",
    "  - ResponseAgent: Final response generation and delivery\n",
    "\n",
    "- **Vector Database Integration**: \n",
    "  - ChromaDB implementation for efficient information storage\n",
    "  - Vector similarity search for relevant information retrieval\n",
    "  - Caching mechanism for improved response times\n",
    "\n",
    "- **Error Handling and Recovery**:  \n",
    "  - Robust error detection and recovery mechanisms\n",
    "  - Fallback options for agent failures\n",
    "  - API retry logic and conflict resolution\n",
    "  \n",
    "- **Evaluation System**: \n",
    "  - Response quality metrics\n",
    "  - User feedback collection and analysis\n",
    "  - System performance monitoring\n",
    "  - Continuous improvement mechanisms\n",
    "\n",
    "### Process Flow\n",
    "\n",
    "1. Query Reception and Analysis\n",
    "   - Natural language query input\n",
    "   - Intent extraction and task categorization\n",
    "   - Timing requirement analysis\n",
    "\n",
    "2. Task Distribution\n",
    "   - Agent selection based on query requirements\n",
    "   - Resource allocation\n",
    "   - Priority assignment\n",
    "\n",
    "3. Information Gathering\n",
    "   - SerpAPI integration for web searches\n",
    "   - Vector database querying\n",
    "   - Information validation and filtering\n",
    "\n",
    "4. Processing and Synthesis\n",
    "   - RAG-based information processing\n",
    "   - Content summarization and recommendation\n",
    "   - Quality assurance checks\n",
    "\n",
    "5. Scheduling and Delivery\n",
    "   - Timing coordination\n",
    "   - Delivery method selection\n",
    "   - Status tracking and updates\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Agent Architecture](#agent-architecture)\n",
    "  - [QueryAnalysisAgent Implementation](#queryanalysisagent-implementation)\n",
    "  - [SearchAgent and SerpAPI Integration](#searchagent-and-serpapi-integration)\n",
    "  - [RAGAgent Implementation](#ragagent-implementation)\n",
    "  - [SchedulerAgent Design](#scheduleragent-design)\n",
    "  - [ResponseAgent Implementation](#responseagent-implementation)\n",
    "- [Vector Database Setup](#vector-database-setup)\n",
    "- [Mail Sending](#mail-sending)\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"chromadb\",\n",
    "        \"langchain_chroma\",\n",
    "        \"langchain_openai\",\n",
    "        \"pytz\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"ResumeRecommendationReview\",\n",
    "        \"UPSTAGE_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Architecture\n",
    "\n",
    "This section explains the structure and roles of each Agent that composes the Multi-Agent Scheduler System. The system consists of five key Agents, each responsible for a specific set of tasks.\n",
    "\n",
    "`QueryAnalysisAgent`\n",
    "Analyzes the user's natural language query to extract time-related information and search requirements. For example, from the query “Please find RAG-related papers and recommend them to me at 7 in the morning,” it identifies the execution time and relevant search keywords.\n",
    "\n",
    "`SearchAgent`\n",
    "Performs web-based searches using SerpAPI. This includes gathering and organizing information from various sources such as arXiv, Papers with Code, GitHub, and more.\n",
    "\n",
    "`RAGAgent`\n",
    "Analyzes and processes the collected information, then generates recommendations that meet the user’s requirements. It integrates with a vector database to facilitate efficient information retrieval and analysis.\n",
    "\n",
    "`SchedulerAgent`\n",
    "Responsible for managing time-based tasks. It executes tasks according to the user’s specified schedule and monitors the status of each task.\n",
    "\n",
    "`ResponseAgent`\n",
    "Formats the processed information so that it can be conveniently delivered to the user. This can include sending the results via email alerts or similar means.\n",
    "\n",
    "----\n",
    "**Agent Flow**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Query] --> B[QueryAnalysisAgent]\n",
    "    B --> C[SearchAgent]\n",
    "    C --> D[RAGAgent]\n",
    "    D --> E[ResponseAgent]\n",
    "    E --> F[User Notification]\n",
    "    B --> G[SchedulerAgent]\n",
    "    G --> E\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QueryAnalysisAgent Implementation\n",
    "The QueryAnalysisAgent is a specialized component of a multi-agent scheduler system designed to parse and analyze user queries. \n",
    "\n",
    "Its primary purpose is to extract timing information and task-related details from natural language inputs, enabling scheduled task execution.\n",
    "\n",
    "Core Functionality\n",
    "The agent performs two main functions:\n",
    "- Time extraction from natural language queries\n",
    "- Task analysis and intent recognition\n",
    "----\n",
    "Usage Example\n",
    "```python\n",
    "agent = QueryAnalysisAgent()\n",
    "query = \"아침 7시에 RAG 관련 논문을 찾아서 추천해줘\"\n",
    "result = agent.analyze_query(query)\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```json\n",
    "{\n",
    "    \"target_time\": \"2025-01-22 07:00:00+0000\",\n",
    "    \"execution_time\": \"2025-01-22 06:55:00+0000\",\n",
    "    \"task_type\": \"추천\",\n",
    "    \"keywords\": [\"아침\", \"7시\", \"RAG\", \"관련\", \"논문\"],\n",
    "    \"requirements\": \"없음\",\n",
    "    \"original_query\": \"아침 7시에 RAG 관련 논문을 찾아서 추천해줘\",\n",
    "    \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "----\n",
    "Integration Points\n",
    "The QueryAnalysisAgent is designed to work seamlessly with:\n",
    "- SchedulerAgent for timing coordination\n",
    "- SearchAgent for executing search tasks\n",
    "- RAGAgent for processing retrieved information\n",
    "- ResponseAgent for formatting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates\n",
    "The agent uses two main prompt templates:\n",
    "\n",
    "a) Time Extraction Prompt:\n",
    "- Purpose: Converts various time formats into 24-hour format\n",
    "- Examples: \n",
    "  - \"아침 7시\" → \"07:00\"\n",
    "  - \"저녁 9시\" → \"21:00\"\n",
    "  - \"오후 3시\" → \"15:00\"\n",
    "\n",
    "b) Task Analysis Prompt:\n",
    "- Purpose: Extracts task information in JSON format\n",
    "- Output Structure:\n",
    "  ```json\n",
    "  {\n",
    "      \"task_type\": \"검색 또는 요약 또는 추천 중 하나\",\n",
    "      \"keywords\": [\"키워드1\", \"키워드2\"],\n",
    "      \"requirements\": \"추가 요구사항\"\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 클래스 정의와 __init__, setup_prompt_templates 메서드\n",
    "class QueryAnalysisAgent:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        # 시간 추출을 위한 프롬프트\n",
    "        self.time_extraction_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            사용자의 쿼리에서 시간 정보를 추출하고 24시간 형식으로 변환하세요.\n",
    "            예시: \n",
    "            - \"아침 7시\" -> \"07:00\"\n",
    "            - \"저녁 9시\" -> \"21:00\"\n",
    "            - \"오후 3시\" -> \"15:00\"\n",
    "            \n",
    "            쿼리: {query}\n",
    "            시간 (HH:MM 형식):\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        # 작업 의도 분석을 위한 프롬프트\n",
    "        self.task_analysis_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            사용자의 쿼리에서 다음 정보를 JSON 형식으로 추출하세요.\n",
    "            \n",
    "            쿼리: {query}\n",
    "            \n",
    "            다음 형식으로 응답해주세요:\n",
    "            {{\n",
    "                \"task_type\": \"검색 또는 요약 또는 추천 중 하나\",\n",
    "                \"keywords\": [\"키워드1\", \"키워드2\"],\n",
    "                \"requirements\": \"추가 요구사항\"\n",
    "            }}\n",
    "            \n",
    "            JSON 형식으로만 응답해주세요.\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 1\n",
    "\n",
    "`extract_time()`\n",
    "- Functionality: Extracts and processes time information from queries\n",
    "- Features:\n",
    "  - Converts various time formats to datetime objects\n",
    "  - Handles timezone awareness using pytz\n",
    "  - Automatically adjusts for next day if time has passed\n",
    "- Error Handling: Raises ValueError for invalid time formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: extract_time 메서드 추가\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"쿼리에서 시간 정보를 추출하고 datetime 객체로 반환\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # ChatCompletion 응답에서 실제 시간 문자열 추출\n",
    "        time_str = time_str.content.strip()\n",
    "\n",
    "        # 현재 시간 기준으로 다음 예정 시간 계산\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        hour, minute = map(int, time_str.split(\":\"))\n",
    "\n",
    "        target_time = current_time.replace(\n",
    "            hour=hour, minute=minute, second=0, microsecond=0\n",
    "        )\n",
    "\n",
    "        # 이미 지난 시간이면 다음 날로 설정\n",
    "        if target_time <= current_time:\n",
    "            target_time += timedelta(days=1)\n",
    "\n",
    "        return target_time\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"시간 추출 실패: {e}\")\n",
    "\n",
    "\n",
    "# Cell 실행 후에는 이 메서드를 클래스에 추가해야 합니다\n",
    "QueryAnalysisAgent.extract_time = extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 2\n",
    "\n",
    "`analyze_task()`\n",
    "- Functionality: Analyzes query content for task information\n",
    "- Features:\n",
    "  - Extracts task type, keywords, and requirements\n",
    "  - Returns structured JSON output\n",
    "  - Maintains context of the original query\n",
    "- Error Handling: Handles JSON parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: analyze_task 메서드 추가\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"쿼리에서 작업 의도와 키워드를 추출\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # ChatCompletion 응답에서 JSON 추출 및 파싱\n",
    "        return json.loads(response.content.strip())\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"작업 분석 결과 파싱 실패: {e}\")\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "QueryAnalysisAgent.analyze_task = analyze_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 3\n",
    "\n",
    "`analyze_query()`\n",
    "- Functionality: Combines time extraction and task analysis\n",
    "- Output Structure:\n",
    "  ```python\n",
    "  {\n",
    "      \"target_time\": datetime,\n",
    "      \"execution_time\": datetime,\n",
    "      \"task_type\": str,\n",
    "      \"keywords\": List[str],\n",
    "      \"requirements\": str,\n",
    "      \"original_query\": str,\n",
    "      \"status\": str\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: analyze_query 메서드 추가\n",
    "def analyze_query(self, query: str) -> dict:\n",
    "    \"\"\"쿼리 전체 분석을 수행하고 결과 반환\"\"\"\n",
    "    try:\n",
    "        target_time = self.extract_time(query)\n",
    "        task_info = self.analyze_task(query)\n",
    "\n",
    "        return {\n",
    "            \"target_time\": target_time,\n",
    "            \"execution_time\": target_time - timedelta(minutes=5),\n",
    "            \"task_type\": task_info[\"task_type\"],\n",
    "            \"keywords\": task_info[\"keywords\"],\n",
    "            \"requirements\": task_info[\"requirements\"],\n",
    "            \"original_query\": query,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"original_query\": query}\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "QueryAnalysisAgent.analyze_query = analyze_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"target_time\": \"2025-01-22 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-01-22 06:55:00+0000\",\n",
      "  \"task_type\": \"추천\",\n",
      "  \"keywords\": [\n",
      "    \"아침\",\n",
      "    \"7시\",\n",
      "    \"RAG\",\n",
      "    \"관련\",\n",
      "    \"논문\"\n",
      "  ],\n",
      "  \"requirements\": \"없음\",\n",
      "  \"original_query\": \"아침 7시에 RAG 관련 논문을 찾아서 추천해줘\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 테스트 (datetime 직렬화 처리 추가)\n",
    "def datetime_handler(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "\n",
    "def test_query():\n",
    "    agent = QueryAnalysisAgent()\n",
    "    query = \"아침 7시에 RAG 관련 논문을 찾아서 추천해줘\"\n",
    "    result = agent.analyze_query(query)\n",
    "    return json.dumps(result, indent=2, ensure_ascii=False, default=datetime_handler)\n",
    "\n",
    "\n",
    "print(test_query())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SearchAgent and SerpAPI Integration\n",
    "\n",
    "\n",
    "The SearchAgent is a specialized component that performs web searches using SerpAPI to gather information related to RAG (Retrieval Augmented Generation). \n",
    "\n",
    "It processes search results into a structured format that can be utilized by other agents in the system.\n",
    "\n",
    "----\n",
    "\n",
    "Key Features\n",
    "\n",
    "- 1. Multi-Source Search\n",
    "- arXiv: Academic paper search\n",
    "- Google Scholar: Scholarly material search\n",
    "- Papers with Code: Papers with implementations\n",
    "- GitHub: Implementation repositories\n",
    "\n",
    "- 2. Result Filtering and Sorting\n",
    "- Relevance score-based filtering\n",
    "- Publication year-based filtering (prioritizing recent materials)\n",
    "- Source-based result integration and sorting\n",
    "\n",
    "----\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize SearchAgent\n",
    "agent = SearchAgent(serpapi_key=\"your_api_key\")\n",
    "\n",
    "# Perform search\n",
    "query_info = {\n",
    "    \"keywords\": [\"RAG\", \"paper\", \"latest\"],\n",
    "    \"original_query\": \"Find RAG related papers\"\n",
    "}\n",
    "\n",
    "results = agent.perform_search(query_info)\n",
    "```\n",
    "\n",
    "### Example Search Result\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"status\": \"success\",\n",
    "    \"results\": [\n",
    "        {\n",
    "            \"type\": \"paper\",\n",
    "            \"source\": \"arxiv\",\n",
    "            \"title\": \"Recent Advances in RAG\",\n",
    "            \"content\": \"This paper discusses recent advances in RAG...\",\n",
    "            \"url\": \"https://arxiv.org/...\",\n",
    "            \"year\": \"2024\",\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    ],\n",
    "    \"total_results_found\": 13,\n",
    "    \"filtered_results_count\": 8,\n",
    "    \"sources_searched\": [\n",
    "        \"arXiv\",\n",
    "        \"Google Scholar\",\n",
    "        \"Papers with Code\",\n",
    "        \"GitHub\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SearchAgent class is designed to perform web searches across multiple academic and technical sources using the SerpAPI wrapper. \n",
    "\n",
    "Here's a detailed breakdown of its implementation:\n",
    "\n",
    "1. `Class Initialization`\n",
    "- Takes an optional SerpAPI key\n",
    "- Sets up the API key as an environment variable if provided\n",
    "- Initializes the SerpAPI wrapper for search operations\n",
    "\n",
    "2. `Search Parameter Setup`\n",
    "This method constructs search queries for different sources:\n",
    "- arXiv for academic papers\n",
    "- Google Scholar for academic research\n",
    "- Papers with Code for implementations\n",
    "- GitHub for code repositories\n",
    "\n",
    "Each source gets a specialized query format. For example:\n",
    "- arXiv: `site:arxiv.org {keywords} RAG Retrieval Augmented Generation`\n",
    "- GitHub: `site:github.com {keywords} RAG implementation`\n",
    "\n",
    "3. `Search Execution`\n",
    "Main features:\n",
    "- Executes searches across all configured sources\n",
    "- Handles each source's results separately\n",
    "- Limits results per source using max_results parameter\n",
    "- Returns a structured response with:\n",
    "  - Search status\n",
    "  - Combined results\n",
    "  - List of sources searched\n",
    "  - Original query reference\n",
    "\n",
    "4. `Result Parsing`\n",
    "Processes results based on source type:\n",
    "- arXiv papers: \n",
    "  ```python\n",
    "  {\n",
    "      \"type\": \"paper\",\n",
    "      \"source\": \"arXiv\",\n",
    "      \"title\": parsed_title,\n",
    "      \"content\": result,\n",
    "      \"url\": extracted_url\n",
    "  }\n",
    "  ```\n",
    "- GitHub implementations:\n",
    "  ```python\n",
    "  {\n",
    "      \"type\": \"implementation\",\n",
    "      \"source\": \"GitHub\",\n",
    "      \"title\": parsed_title,\n",
    "      \"content\": result,\n",
    "      \"url\": extracted_url\n",
    "  }\n",
    "  ```\n",
    "- Other sources:\n",
    "  ```python\n",
    "  {\n",
    "      \"type\": \"reference\",\n",
    "      \"source\": source_name,\n",
    "      \"title\": parsed_title,\n",
    "      \"content\": result,\n",
    "      \"url\": extracted_url\n",
    "  }\n",
    "  ```\n",
    "\n",
    "5. `URL Extraction`\n",
    "- Placeholder method for URL extraction\n",
    "- Intended to be implemented with regex or other parsing logic\n",
    "- Currently returns empty string as temporary implementation\n",
    "----\n",
    "Return Format\n",
    "Successful search result:\n",
    "```python\n",
    "{\n",
    "    \"status\": \"success\",\n",
    "    \"results\": [parsed_results],\n",
    "    \"sources_searched\": [\"arXiv\", \"Google Scholar\", \"Papers with Code\", \"GitHub\"],\n",
    "    \"original_query\": original_query\n",
    "}\n",
    "```\n",
    "\n",
    "Error response:\n",
    "```python\n",
    "{\n",
    "    \"status\": \"error\",\n",
    "    \"error_message\": error_description,\n",
    "    \"original_query\": original_query\n",
    "}\n",
    "```\n",
    "\n",
    "This implementation provides a robust framework for executing multi-source academic and technical searches, with structured result parsing and error handling.\n",
    "\n",
    " It's particularly focused on RAG-related content across academic papers and technical implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchAgent:\n",
    "    def __init__(self, serpapi_key: str = None):\n",
    "        if serpapi_key:\n",
    "            os.environ[\"SERPAPI_API_KEY\"] = serpapi_key\n",
    "        self.search = SerpAPIWrapper()\n",
    "\n",
    "    def setup_search_parameters(self, query_info: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"검색 전략 수립\"\"\"\n",
    "        search_queries = []\n",
    "\n",
    "        # 학술 논문 검색을 위한 키워드 구성\n",
    "        if \"논문\" in query_info[\"keywords\"]:\n",
    "            # arXiv 검색\n",
    "            search_queries.append(\n",
    "                f\"site:arxiv.org {' '.join(query_info['keywords'])} RAG Retrieval Augmented Generation\"\n",
    "            )\n",
    "\n",
    "            # Google Scholar 검색\n",
    "            search_queries.append(\n",
    "                f\"site:scholar.google.com {' '.join(query_info['keywords'])} RAG LLM\"\n",
    "            )\n",
    "\n",
    "            # Papers with Code 검색\n",
    "            search_queries.append(\n",
    "                f\"site:paperswithcode.com {' '.join(query_info['keywords'])} RAG\"\n",
    "            )\n",
    "\n",
    "            # 깃허브 구현체 검색\n",
    "            search_queries.append(\n",
    "                f\"site:github.com {' '.join(query_info['keywords'])} RAG implementation\"\n",
    "            )\n",
    "\n",
    "        return search_queries\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"여러 소스에서 검색 수행\"\"\"\n",
    "        try:\n",
    "            search_queries = self.setup_search_parameters(query_info)\n",
    "            all_results = []\n",
    "\n",
    "            for query in search_queries:\n",
    "                raw_results = self.search.run(query)\n",
    "\n",
    "                # 소스별 결과 파싱\n",
    "                source = (\n",
    "                    \"arxiv\"\n",
    "                    if \"arxiv.org\" in query\n",
    "                    else (\n",
    "                        \"scholar\"\n",
    "                        if \"scholar.google.com\" in query\n",
    "                        else (\n",
    "                            \"papers_with_code\"\n",
    "                            if \"paperswithcode.com\" in query\n",
    "                            else \"github\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # 결과 정리 및 메타데이터 추가\n",
    "                parsed_results = self._parse_results(raw_results, source)\n",
    "                all_results.extend(parsed_results[:max_results])\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": all_results,\n",
    "                \"sources_searched\": [\n",
    "                    \"arXiv\",\n",
    "                    \"Google Scholar\",\n",
    "                    \"Papers with Code\",\n",
    "                    \"GitHub\",\n",
    "                ],\n",
    "                \"original_query\": query_info[\"original_query\"],\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"original_query\": query_info[\"original_query\"],\n",
    "            }\n",
    "\n",
    "    def _parse_results(self, raw_results: str, source: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"검색 결과 파싱 및 구조화\"\"\"\n",
    "        parsed_results = []\n",
    "        for result in raw_results.split(\"\\n\"):\n",
    "            if result.strip():\n",
    "                # 소스별 파싱 로직\n",
    "                if source == \"arxiv\":\n",
    "                    # arXiv 논문 정보 추출\n",
    "                    parsed_results.append(\n",
    "                        {\n",
    "                            \"type\": \"paper\",\n",
    "                            \"source\": \"arXiv\",\n",
    "                            \"title\": (\n",
    "                                result.split(\" - \")[0] if \" - \" in result else result\n",
    "                            ),\n",
    "                            \"content\": result,\n",
    "                            \"url\": self._extract_url(result),\n",
    "                        }\n",
    "                    )\n",
    "                elif source == \"github\":\n",
    "                    # GitHub 구현체 정보 추출\n",
    "                    parsed_results.append(\n",
    "                        {\n",
    "                            \"type\": \"implementation\",\n",
    "                            \"source\": \"GitHub\",\n",
    "                            \"title\": (\n",
    "                                result.split(\" - \")[0] if \" - \" in result else result\n",
    "                            ),\n",
    "                            \"content\": result,\n",
    "                            \"url\": self._extract_url(result),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    # 기타 소스 정보 추출\n",
    "                    parsed_results.append(\n",
    "                        {\n",
    "                            \"type\": \"reference\",\n",
    "                            \"source\": source,\n",
    "                            \"title\": (\n",
    "                                result.split(\" - \")[0] if \" - \" in result else result\n",
    "                            ),\n",
    "                            \"content\": result,\n",
    "                            \"url\": self._extract_url(result),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return parsed_results\n",
    "\n",
    "    def _extract_url(self, result: str) -> str:\n",
    "        \"\"\"결과에서 URL 추출\"\"\"\n",
    "        # URL 추출 로직 구현\n",
    "        # 실제 구현에서는 정규식 등을 사용하여 URL 추출\n",
    "        return \"\"  # 임시 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Result Utility Functions\n",
    "\n",
    "1. `clean_and_parse_results`\n",
    "Purpose: Cleans and parses raw search result strings into a structured format.\n",
    "\n",
    "Key features:\n",
    "- Handles string-to-list conversion\n",
    "- Validates string format\n",
    "- Uses safe evaluation of string content\n",
    "\n",
    "2. `extract_url_from_content`\n",
    "Purpose: Extracts URLs from content using regex pattern matching.\n",
    "\n",
    "Features:\n",
    "- Uses regex pattern for URL detection\n",
    "- Handles HTTP and HTTPS URLs\n",
    "- Returns first found URL or empty string\n",
    "\n",
    "3. `extract_year`\n",
    "Purpose: Extracts year information from content text.\n",
    "\n",
    "Features:\n",
    "- Extracts years starting with '20'\n",
    "- Returns most recent year if multiple found\n",
    "- Returns empty string if no year found\n",
    "\n",
    "----\n",
    "Usage Context\n",
    "These utility functions are used within the SearchAgent class to:\n",
    "1. Process raw search results from SerpAPI\n",
    "2. Extract metadata from search results\n",
    "3. Standardize result format for further processing\n",
    "\n",
    "This set of utility functions provides robust preprocessing of search results, ensuring clean and structured data for the rest of the system to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 검색 결과 처리를 위한 유틸리티 함수 추가\n",
    "def _clean_and_parse_results(self, raw_results: str) -> List[str]:\n",
    "    \"\"\"검색 결과 문자열을 깔끔하게 파싱\"\"\"\n",
    "    if isinstance(raw_results, str):\n",
    "        # 문자열이 리스트처럼 생겼다면 실제 리스트로 변환\n",
    "        if raw_results.startswith(\"[\") and raw_results.endswith(\"]\"):\n",
    "            try:\n",
    "                # 문자열을 실제 파이썬 객체로 변환\n",
    "                results = eval(raw_results)\n",
    "                if isinstance(results, list):\n",
    "                    return results\n",
    "            except:\n",
    "                pass\n",
    "    return [raw_results]\n",
    "\n",
    "\n",
    "def _extract_url_from_content(self, content: str) -> str:\n",
    "    \"\"\"컨텐츠에서 URL 추출\"\"\"\n",
    "    import re\n",
    "\n",
    "    url_pattern = r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n",
    "    urls = re.findall(url_pattern, content)\n",
    "    return urls[0] if urls else \"\"\n",
    "\n",
    "\n",
    "def _extract_year(self, content: str) -> str:\n",
    "    \"\"\"컨텐츠에서 연도 추출\"\"\"\n",
    "    import re\n",
    "\n",
    "    year_pattern = r\"(20\\d{2})\"\n",
    "    years = re.findall(year_pattern, content)\n",
    "    return max(years) if years else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SearchAgent's perform_search Method Documentation\n",
    "\n",
    "A method that performs searches across multiple academic and technical sources and processes the results.\n",
    "\n",
    "Key Features\n",
    "\n",
    "1. `Search Execution`\n",
    "- Generates search queries using setup_search_parameters\n",
    "- Initializes list for storing results\n",
    "\n",
    "2. `Source-Specific Processing`\n",
    "- Executes each search query\n",
    "- Cleans and parses results\n",
    "\n",
    "3. `Source Identification`\n",
    "- Identifies source based on query URL\n",
    "- Supports arXiv, Google Scholar, Papers with Code, GitHub\n",
    "\n",
    "4. `Result Entry Creation`\n",
    "- Creates structured result data\n",
    "- Calculates relevance score (based on RAG presence)\n",
    "\n",
    "5. `Result Filtering and Sorting`\n",
    "- Filters results with relevance score > 0.5\n",
    "- Includes results from 2022 or newer\n",
    "- Sorts by relevance and year\n",
    "\n",
    "This method effectively performs RAG-related searches and systematically processes results into a usable format, making it a valuable component of the search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: perform_search 메서드 개선\n",
    "def perform_search(\n",
    "    self, query_info: Dict[str, Any], max_results: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"여러 소스에서 검색 수행\"\"\"\n",
    "    try:\n",
    "        search_queries = self.setup_search_parameters(query_info)\n",
    "        all_results = []\n",
    "\n",
    "        for query in search_queries:\n",
    "            try:\n",
    "                # SerpAPI 검색 실행\n",
    "                raw_results = self.search.run(query)\n",
    "                results_list = self._clean_and_parse_results(raw_results)\n",
    "\n",
    "                # 소스 확인\n",
    "                source = (\n",
    "                    \"arxiv\"\n",
    "                    if \"arxiv.org\" in query\n",
    "                    else (\n",
    "                        \"scholar\"\n",
    "                        if \"scholar.google.com\" in query\n",
    "                        else (\n",
    "                            \"papers_with_code\"\n",
    "                            if \"paperswithcode.com\" in query\n",
    "                            else \"github\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # 각 결과 처리\n",
    "                for result in results_list:\n",
    "                    url = self._extract_url_from_content(result)\n",
    "                    year = self._extract_year(result)\n",
    "\n",
    "                    # 결과 정리\n",
    "                    result_entry = {\n",
    "                        \"type\": (\n",
    "                            \"paper\"\n",
    "                            if source in [\"arxiv\", \"scholar\"]\n",
    "                            else \"implementation\"\n",
    "                        ),\n",
    "                        \"source\": source,\n",
    "                        \"title\": (\n",
    "                            result[: result.find(\".\")]\n",
    "                            if \".\" in result\n",
    "                            else result[:100]\n",
    "                        ),\n",
    "                        \"content\": result,\n",
    "                        \"url\": url,\n",
    "                        \"year\": year,\n",
    "                        \"relevance_score\": 1 if \"RAG\" in result.upper() else 0.5,\n",
    "                    }\n",
    "\n",
    "                    all_results.append(result_entry)\n",
    "\n",
    "            except Exception as query_error:\n",
    "                print(f\"Query '{query}' failed: {str(query_error)}\")\n",
    "                continue\n",
    "\n",
    "        # 결과 정렬 및 필터링\n",
    "        filtered_results = [\n",
    "            r\n",
    "            for r in all_results\n",
    "            if r[\"relevance_score\"] > 0.5\n",
    "            and (\n",
    "                not r[\"year\"] or int(r[\"year\"]) >= 2022\n",
    "            )  # 2022년 이후 또는 연도 없는 결과\n",
    "        ]\n",
    "\n",
    "        sorted_results = sorted(\n",
    "            filtered_results,\n",
    "            key=lambda x: (x[\"relevance_score\"], x[\"year\"] if x[\"year\"] else \"0\"),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"results\": sorted_results[:max_results],\n",
    "            \"total_results_found\": len(all_results),\n",
    "            \"filtered_results_count\": len(filtered_results),\n",
    "            \"sources_searched\": [\n",
    "                \"arXiv\",\n",
    "                \"Google Scholar\",\n",
    "                \"Papers with Code\",\n",
    "                \"GitHub\",\n",
    "            ],\n",
    "            \"original_query\": query_info[\"original_query\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"original_query\": query_info.get(\"original_query\", \"\"),\n",
    "        }\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "SearchAgent._clean_and_parse_results = _clean_and_parse_results\n",
    "SearchAgent._extract_url_from_content = _extract_url_from_content\n",
    "SearchAgent._extract_year = _extract_year\n",
    "SearchAgent.perform_search = perform_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 'site:scholar.google.com RAG 논문 최신 RAG LLM' failed: Got error from SerpAPI: Google hasn't returned any results for this query.\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"type\": \"implementation\",\n",
      "      \"source\": \"github\",\n",
      "      \"title\": \"논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 \",\n",
      "      \"content\": \"논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 ...\",\n",
      "      \"url\": \"\",\n",
      "      \"year\": \"2024\",\n",
      "      \"relevance_score\": 1\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"paper\",\n",
      "      \"source\": \"arxiv\",\n",
      "      \"title\": \"Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search \",\n",
      "      \"content\": \"Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search ...\",\n",
      "      \"url\": \"\",\n",
      "      \"year\": \"\",\n",
      "      \"relevance_score\": 1\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"paper\",\n",
      "      \"source\": \"arxiv\",\n",
      "      \"title\": \"To evaluate the risk of multilingual harmful content generation, we augmented \",\n",
      "      \"content\": \"To evaluate the risk of multilingual harmful content generation, we augmented ... a technique known as Retrieval Augmented Generation (i.e., “RAG”) ...\",\n",
      "      \"url\": \"\",\n",
      "      \"year\": \"\",\n",
      "      \"relevance_score\": 1\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"implementation\",\n",
      "      \"source\": \"github\",\n",
      "      \"title\": \"PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 \",\n",
      "      \"content\": \"PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 ...\",\n",
      "      \"url\": \"\",\n",
      "      \"year\": \"\",\n",
      "      \"relevance_score\": 1\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"implementation\",\n",
      "      \"source\": \"github\",\n",
      "      \"title\": \"Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20\",\n",
      "      \"content\": \"Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20.05.22 ; Hugging Face, DistilBERT, DistilBERT ...\",\n",
      "      \"url\": \"\",\n",
      "      \"year\": \"\",\n",
      "      \"relevance_score\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"total_results_found\": 13,\n",
      "  \"filtered_results_count\": 8,\n",
      "  \"sources_searched\": [\n",
      "    \"arXiv\",\n",
      "    \"Google Scholar\",\n",
      "    \"Papers with Code\",\n",
      "    \"GitHub\"\n",
      "  ],\n",
      "  \"original_query\": \"논문 Find RAG related paper\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 테스트\n",
    "def test_search():\n",
    "    # QueryAnalysisAgent에서 받은 결과를 시뮬레이션\n",
    "    query_info = {\n",
    "        \"keywords\": [\"RAG\", \"논문\", \"최신\"],\n",
    "        \"original_query\": \"논문 Find RAG related paper\",\n",
    "    }\n",
    "\n",
    "    agent = SearchAgent()\n",
    "    results = agent.perform_search(query_info)\n",
    "    return json.dumps(results, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "print(test_search())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 'site:scholar.google.com RAG 논문 최신 RAG LLM' failed: Got error from SerpAPI: Google hasn't returned any results for this query.\n"
     ]
    }
   ],
   "source": [
    "data = test_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"status\": \"success\",\\n  \"results\": [\\n    {\\n      \"type\": \"implementation\",\\n      \"source\": \"github\",\\n      \"title\": \"논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 \",\\n      \"content\": \"논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 ...\",\\n      \"url\": \"\",\\n      \"year\": \"2024\",\\n      \"relevance_score\": 1\\n    },\\n    {\\n      \"type\": \"paper\",\\n      \"source\": \"arxiv\",\\n      \"title\": \"Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search \",\\n      \"content\": \"Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search ...\",\\n      \"url\": \"\",\\n      \"year\": \"\",\\n      \"relevance_score\": 1\\n    },\\n    {\\n      \"type\": \"paper\",\\n      \"source\": \"arxiv\",\\n      \"title\": \"To evaluate the risk of multilingual harmful content generation, we augmented \",\\n      \"content\": \"To evaluate the risk of multilingual harmful content generation, we augmented ... a technique known as Retrieval Augmented Generation (i.e., “RAG”) ...\",\\n      \"url\": \"\",\\n      \"year\": \"\",\\n      \"relevance_score\": 1\\n    },\\n    {\\n      \"type\": \"implementation\",\\n      \"source\": \"github\",\\n      \"title\": \"PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 \",\\n      \"content\": \"PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 ...\",\\n      \"url\": \"\",\\n      \"year\": \"\",\\n      \"relevance_score\": 1\\n    },\\n    {\\n      \"type\": \"implementation\",\\n      \"source\": \"github\",\\n      \"title\": \"Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20\",\\n      \"content\": \"Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20.05.22 ; Hugging Face, DistilBERT, DistilBERT ...\",\\n      \"url\": \"\",\\n      \"year\": \"\",\\n      \"relevance_score\": 1\\n    }\\n  ],\\n  \"total_results_found\": 13,\\n  \"filtered_results_count\": 8,\\n  \"sources_searched\": [\\n    \"arXiv\",\\n    \"Google Scholar\",\\n    \"Papers with Code\",\\n    \"GitHub\"\\n  ],\\n  \"original_query\": \"논문 Find RAG related paper\"\\n}'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 문자열을 파이썬 딕셔너리로 변환하고 results 키의 값을 가져옴\n",
    "results = json.loads(data)[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'implementation',\n",
       "  'source': 'github',\n",
       "  'title': '논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 ',\n",
       "  'content': '논문 리뷰와 코드를 준비했습니다 - YouTube · RAG관련 최신 논문 RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval(2024) · VidiGo | 비디고 ...',\n",
       "  'url': '',\n",
       "  'year': '2024',\n",
       "  'relevance_score': 1},\n",
       " {'type': 'paper',\n",
       "  'source': 'arxiv',\n",
       "  'title': 'Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search ',\n",
       "  'content': 'Retrieval-augmented generation (RAG) addresses these limitations by integrating an external knowledge source, such as a database or search ...',\n",
       "  'url': '',\n",
       "  'year': '',\n",
       "  'relevance_score': 1},\n",
       " {'type': 'paper',\n",
       "  'source': 'arxiv',\n",
       "  'title': 'To evaluate the risk of multilingual harmful content generation, we augmented ',\n",
       "  'content': 'To evaluate the risk of multilingual harmful content generation, we augmented ... a technique known as Retrieval Augmented Generation (i.e., “RAG”) ...',\n",
       "  'url': '',\n",
       "  'year': '',\n",
       "  'relevance_score': 1},\n",
       " {'type': 'implementation',\n",
       "  'source': 'github',\n",
       "  'title': 'PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 ',\n",
       "  'content': 'PDF 또는 텍스트 파일 대상으로 RAG를 수행하여 논문을 쉽게 읽을 수 있도록 도와주는 패키지; QA, 요약, contradiction detection 등 가능; pip install paper-qa; 논문 ...',\n",
       "  'url': '',\n",
       "  'year': '',\n",
       "  'relevance_score': 1},\n",
       " {'type': 'implementation',\n",
       "  'source': 'github',\n",
       "  'title': 'Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20',\n",
       "  'content': 'Pre-trained NLP Architecture ; FAIR, RAG, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, arxiv, 20.05.22 ; Hugging Face, DistilBERT, DistilBERT ...',\n",
       "  'url': '',\n",
       "  'year': '',\n",
       "  'relevance_score': 1}]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAgent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from typing import List, Dict, Any\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: RAGAgent 클래스 및 기본 메서드 정의\n",
    "class RAGAgent:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        self.analysis_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Analyze the following research papers related to RAG (Retrieval Augmented Generation) and provide a comprehensive summary and recommendations.\n",
    "            \n",
    "            Papers:\n",
    "            {papers}\n",
    "            \n",
    "            Please provide:\n",
    "            1. Brief summary of each paper\n",
    "            2. Key innovations and contributions\n",
    "            3. Potential applications\n",
    "            4. Recommendations based on user's interests\n",
    "            \n",
    "            Format your response as JSON with the following structure:\n",
    "            {{\n",
    "                \"summaries\": [\n",
    "                    {{\"title\": \"\", \"summary\": \"\", \"key_points\": []}}\n",
    "                ],\n",
    "                \"recommendations\": [\n",
    "                    {{\"title\": \"\", \"reason\": \"\"}}\n",
    "                ],\n",
    "                \"overall_analysis\": \"\"\n",
    "            }}\n",
    "            \n",
    "            Original user query: {original_query}\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 데이터 처리 메서드\n",
    "def preprocess_papers(\n",
    "    self, search_results: List[Dict[str, Any]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"검색 결과를 분석하기 좋은 형태로 전처리\"\"\"\n",
    "    processed_papers = []\n",
    "    for result in search_results:\n",
    "        processed_papers.append(\n",
    "            {\n",
    "                \"title\": result[\"title\"],\n",
    "                \"content\": result[\"content\"],\n",
    "                \"year\": result.get(\"year\", \"\"),\n",
    "                \"source\": result[\"source\"],\n",
    "                \"url\": result[\"url\"],\n",
    "                \"relevance_score\": result[\"relevance_score\"],\n",
    "            }\n",
    "        )\n",
    "    return processed_papers\n",
    "\n",
    "\n",
    "RAGAgent.preprocess_papers = preprocess_papers\n",
    "\n",
    "\n",
    "def create_vector_store(self, papers: List[Dict[str, Any]]):\n",
    "    \"\"\"논문 내용을 벡터 스토어에 저장\"\"\"\n",
    "    texts = [paper[\"content\"] for paper in papers]\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"source\": paper[\"source\"],\n",
    "            \"url\": paper[\"url\"],\n",
    "        }\n",
    "        for paper in papers\n",
    "    ]\n",
    "\n",
    "    return Chroma.from_texts(\n",
    "        texts=texts, embedding=self.embeddings, metadatas=metadatas\n",
    "    )\n",
    "\n",
    "\n",
    "RAGAgent.create_vector_store = create_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: analyze_and_recommend 메서드 추가\n",
    "def analyze_and_recommend(\n",
    "    self, papers: List[Dict[str, Any]], query_info: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"논문 분석 및 추천 수행\"\"\"\n",
    "    try:\n",
    "        # 벡터 스토어 생성\n",
    "        vector_store = self.create_vector_store(papers)\n",
    "\n",
    "        # 분석을 위한 컨텍스트 준비\n",
    "        papers_context = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"Title: {paper['title']}\\nContent: {paper['content']}\\nYear: {paper['year']}\\nSource: {paper['source']}\"\n",
    "                for paper in papers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # LLM을 사용한 분석 수행\n",
    "        analysis_chain = self.analysis_prompt | self.llm\n",
    "        analysis_result = analysis_chain.invoke(\n",
    "            {\"papers\": papers_context, \"original_query\": query_info[\"original_query\"]}\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # JSON 파싱\n",
    "            result = json.loads(analysis_result.content)\n",
    "            result[\"status\"] = \"success\"\n",
    "            return result\n",
    "        except json.JSONDecodeError:\n",
    "            # JSON 파싱 실패 시 원본 텍스트 반환\n",
    "            return {\n",
    "                \"status\": \"partial_success\",\n",
    "                \"raw_analysis\": analysis_result.content,\n",
    "                \"error\": \"JSON parsing failed\",\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "RAGAgent.analyze_and_recommend = analyze_and_recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: process_papers 메서드 추가\n",
    "def process_papers(\n",
    "    self, search_results: List[Dict[str, Any]], query_info: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"전체 논문 처리 프로세스 실행\"\"\"\n",
    "    try:\n",
    "        # 데이터 전처리\n",
    "        processed_papers = self.preprocess_papers(search_results)\n",
    "\n",
    "        # 분석 및 추천\n",
    "        analysis_results = self.analyze_and_recommend(processed_papers, query_info)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"analysis_results\": analysis_results,\n",
    "            \"papers_processed\": len(processed_papers),\n",
    "            \"original_query\": query_info[\"original_query\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"original_query\": query_info[\"original_query\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "RAGAgent.process_papers = process_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"analysis_results\": {\n",
      "    \"summaries\": [\n",
      "      {\n",
      "        \"title\": \"Recent Advances in RAG\",\n",
      "        \"summary\": \"This paper discusses recent advances in Retrieval Augmented Generation...\",\n",
      "        \"key_points\": [\n",
      "          \"Discusses recent advances in RAG\",\n",
      "          \"Provides insights into the latest developments in the field\",\n",
      "          \"Highlights key research findings and trends\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"recommendations\": [\n",
      "      {\n",
      "        \"title\": \"Recent Advances in RAG\",\n",
      "        \"reason\": \"This paper provides a comprehensive overview of the recent advancements in RAG, making it a valuable resource for anyone interested in staying up-to-date with the latest developments in the field.\"\n",
      "      }\n",
      "    ],\n",
      "    \"overall_analysis\": \"The paper on Recent Advances in RAG is a valuable resource for researchers, practitioners, and anyone interested in the field of Retrieval Augmented Generation. It offers insights into the latest trends, key findings, and potential applications of RAG, making it a recommended read for those looking to expand their knowledge in this area.\",\n",
      "    \"status\": \"success\"\n",
      "  },\n",
      "  \"papers_processed\": 1,\n",
      "  \"original_query\": \"Find RAG related papers at 7 AM and recommend them\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 테스트\n",
    "def test_rag_agent():\n",
    "    # SearchAgent 결과 시뮬레이션\n",
    "    search_results = [\n",
    "        {\n",
    "            \"title\": \"Recent Advances in RAG\",\n",
    "            \"content\": \"This paper discusses recent advances in Retrieval Augmented Generation...\",\n",
    "            \"year\": \"2024\",\n",
    "            \"source\": \"arxiv\",\n",
    "            \"url\": \"https://example.com\",\n",
    "            \"relevance_score\": 1,\n",
    "        },\n",
    "        # 더 많은 테스트 데이터 추가 가능\n",
    "    ]\n",
    "\n",
    "    query_info = {\n",
    "        \"keywords\": [\"RAG\", \"paper\", \"latest\"],\n",
    "        \"original_query\": \"Find RAG related papers at 7 AM and recommend them\",\n",
    "    }\n",
    "\n",
    "    agent = RAGAgent()\n",
    "    results = agent.process_papers(search_results, query_info)\n",
    "    return json.dumps(results, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "print(test_rag_agent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SchedulerAgent Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 필요한 라이브러리 임포트\n",
    "import asyncio\n",
    "from typing import Dict, Any, Optional, List, Callable\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: SchedulerAgent 클래스의 schedule_task 메서드 수정\n",
    "class SchedulerAgent:\n",
    "    def __init__(self):\n",
    "        self.scheduled_tasks: Dict[str, Dict[str, Any]] = {}\n",
    "        self.task_queue = asyncio.Queue()\n",
    "\n",
    "    def schedule_task(\n",
    "        self,\n",
    "        task_id: str,\n",
    "        target_time: datetime,\n",
    "        task_info: Dict[str, Any],\n",
    "        callback: Optional[Callable] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"작업 스케줄링\"\"\"\n",
    "        try:\n",
    "            current_time = datetime.now(pytz.utc)\n",
    "            # 목표 시간이 현재보다 이전인 경우\n",
    "            if target_time <= current_time:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Target time ({target_time}) must be in the future. Current time: {current_time}\",\n",
    "                    \"task_id\": task_id,\n",
    "                }\n",
    "\n",
    "            # 실행 시간 계산 (목표 시간 5분 전)\n",
    "            execution_time = target_time - timedelta(minutes=5)\n",
    "\n",
    "            # 실행 시간이 현재보다 이전인 경우, 바로 실행하도록 현재 시간으로 설정\n",
    "            if execution_time <= current_time:\n",
    "                execution_time = current_time\n",
    "\n",
    "            print(f\"Task Scheduling Details:\")\n",
    "            print(f\"- Current Time: {current_time}\")\n",
    "            print(f\"- Execution Time: {execution_time}\")\n",
    "            print(f\"- Target Time: {target_time}\")\n",
    "\n",
    "            # 작업 정보 저장\n",
    "            self.scheduled_tasks[task_id] = {\n",
    "                \"task_id\": task_id,\n",
    "                \"target_time\": target_time,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"task_info\": task_info,\n",
    "                \"callback\": callback,\n",
    "                \"status\": \"scheduled\",\n",
    "                \"result\": None,\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Task scheduled successfully\",\n",
    "                \"task_id\": task_id,\n",
    "                \"scheduled_time\": target_time.isoformat(),\n",
    "                \"execution_time\": execution_time.isoformat(),\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"error_message\": str(e), \"task_id\": task_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 작업 실행 관련 메서드\n",
    "async def execute_task(self, task_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"작업 실행\"\"\"\n",
    "    try:\n",
    "        task = self.scheduled_tasks[task_id]\n",
    "\n",
    "        # 이미 완료된 작업이면 건너뛰기\n",
    "        if task[\"status\"] == \"completed\":\n",
    "            return {\n",
    "                \"status\": \"skipped\",\n",
    "                \"message\": \"Task already completed\",\n",
    "                \"task_id\": task_id,\n",
    "            }\n",
    "\n",
    "        # 실행 시간 확인\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        if current_time < task[\"execution_time\"]:\n",
    "            return {\n",
    "                \"status\": \"waiting\",\n",
    "                \"message\": \"Not yet execution time\",\n",
    "                \"task_id\": task_id,\n",
    "            }\n",
    "\n",
    "        # 상태 업데이트\n",
    "        task[\"status\"] = \"executing\"\n",
    "        print(f\"Executing task {task_id} at {current_time}\")\n",
    "\n",
    "        # 콜백 함수 실행\n",
    "        if task[\"callback\"]:\n",
    "            result = await task[\"callback\"](task[\"task_info\"])\n",
    "            task[\"result\"] = result\n",
    "\n",
    "        # 작업 완료 처리\n",
    "        task[\"status\"] = \"completed\"\n",
    "        task[\"completed_at\"] = datetime.now(pytz.utc)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"task_id\": task_id,\n",
    "            \"execution_result\": task[\"result\"],\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        if task_id in self.scheduled_tasks:\n",
    "            self.scheduled_tasks[task_id][\"status\"] = \"failed\"\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"task_id\": task_id}\n",
    "\n",
    "\n",
    "async def monitor_tasks(self):\n",
    "    \"\"\"작업 모니터링 및 실행\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            current_time = datetime.now(pytz.utc)\n",
    "\n",
    "            # 실행할 작업 확인\n",
    "            for task_id, task in list(self.scheduled_tasks.items()):\n",
    "                if (\n",
    "                    task[\"status\"] == \"scheduled\"\n",
    "                    and current_time >= task[\"execution_time\"]\n",
    "                    and task_id not in self.task_queue._queue\n",
    "                ):\n",
    "                    print(f\"Adding task {task_id} to queue at {current_time}\")\n",
    "                    await self.task_queue.put(task_id)\n",
    "\n",
    "            await asyncio.sleep(1)  # 1초마다 체크\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in monitor_tasks: {e}\")\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "\n",
    "async def process_task_queue(self):\n",
    "    \"\"\"작업 큐 처리\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # 큐에서 작업 가져오기\n",
    "            task_id = await self.task_queue.get()\n",
    "\n",
    "            # 작업 실행\n",
    "            await self.execute_task(task_id)\n",
    "\n",
    "            # 작업 완료 표시\n",
    "            self.task_queue.task_done()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_task_queue: {e}\")\n",
    "\n",
    "        await asyncio.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 상태 확인 메서드\n",
    "def get_task_status(self, task_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"작업 상태 조회\"\"\"\n",
    "    if task_id in self.scheduled_tasks:\n",
    "        task = self.scheduled_tasks[task_id]\n",
    "        status_info = {\n",
    "            \"status\": \"success\",\n",
    "            \"task_info\": {\n",
    "                \"task_id\": task_id,\n",
    "                \"current_status\": task[\"status\"],\n",
    "                \"target_time\": task[\"target_time\"].isoformat(),\n",
    "                \"execution_time\": task[\"execution_time\"].isoformat(),\n",
    "                \"result\": task.get(\"result\"),\n",
    "            },\n",
    "        }\n",
    "        # 완료 시간이 있으면 추가\n",
    "        if \"completed_at\" in task:\n",
    "            status_info[\"task_info\"][\"completed_at\"] = task[\"completed_at\"].isoformat()\n",
    "        return status_info\n",
    "\n",
    "    return {\"status\": \"error\", \"error_message\": \"Task not found\", \"task_id\": task_id}\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "SchedulerAgent.execute_task = execute_task\n",
    "SchedulerAgent.monitor_tasks = monitor_tasks\n",
    "SchedulerAgent.process_task_queue = process_task_queue\n",
    "SchedulerAgent.get_task_status = get_task_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Scheduling Details:\n",
      "- Current Time: 2025-01-21 15:04:10.929193+00:00\n",
      "- Execution Time: 2025-01-21 15:04:10.929193+00:00\n",
      "- Target Time: 2025-01-21 15:04:40.929116+00:00\n",
      "\n",
      "Schedule Result: {\n",
      "  \"status\": \"success\",\n",
      "  \"message\": \"Task scheduled successfully\",\n",
      "  \"task_id\": \"test_task_1\",\n",
      "  \"scheduled_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "  \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\"\n",
      "}\n",
      "Adding task test_task_1 to queue at 2025-01-21 15:04:10.929547+00:00\n",
      "Executing task test_task_1 at 2025-01-21 15:04:10.929593+00:00\n",
      "\n",
      "Task Status at 2025-01-21 15:04:15.930575+00:00 (Check 1/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Task Status at 2025-01-21 15:04:20.931895+00:00 (Check 2/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Task Status at 2025-01-21 15:04:25.932555+00:00 (Check 3/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Task Status at 2025-01-21 15:04:30.934136+00:00 (Check 4/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Task Status at 2025-01-21 15:04:35.935679+00:00 (Check 5/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Task Status at 2025-01-21 15:04:40.937313+00:00 (Check 6/6):\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"task_info\": {\n",
      "    \"task_id\": \"test_task_1\",\n",
      "    \"current_status\": \"completed\",\n",
      "    \"target_time\": \"2025-01-21T15:04:40.929116+00:00\",\n",
      "    \"execution_time\": \"2025-01-21T15:04:10.929193+00:00\",\n",
      "    \"result\": {\n",
      "      \"status\": \"success\",\n",
      "      \"message\": \"Processed task with info: {'query': 'Find RAG related papers', 'keywords': ['RAG', 'paper', 'latest']}\"\n",
      "    },\n",
      "    \"completed_at\": \"2025-01-21T15:04:12.930328+00:00\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: 테스트 코드\n",
    "async def test_callback(task_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    await asyncio.sleep(2)  # 작업 시뮬레이션\n",
    "    return {\"status\": \"success\", \"message\": f\"Processed task with info: {task_info}\"}\n",
    "\n",
    "\n",
    "def run_scheduler_test():\n",
    "    async def test_scheduler():\n",
    "        scheduler = SchedulerAgent()\n",
    "\n",
    "        # 현재 시간 + 30초로 목표 시간 설정\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        target_time = current_time + timedelta(seconds=30)\n",
    "\n",
    "        task_info = {\n",
    "            \"query\": \"Find RAG related papers\",\n",
    "            \"keywords\": [\"RAG\", \"paper\", \"latest\"],\n",
    "        }\n",
    "\n",
    "        # 작업 스케줄링\n",
    "        schedule_result = scheduler.schedule_task(\n",
    "            task_id=\"test_task_1\",\n",
    "            target_time=target_time,\n",
    "            task_info=task_info,\n",
    "            callback=test_callback,\n",
    "        )\n",
    "\n",
    "        print(\"\\nSchedule Result:\", json.dumps(schedule_result, indent=2))\n",
    "\n",
    "        # 모니터링 및 큐 처리 시작\n",
    "        monitoring_task = asyncio.create_task(scheduler.monitor_tasks())\n",
    "        queue_processing_task = asyncio.create_task(scheduler.process_task_queue())\n",
    "\n",
    "        # 상태를 주기적으로 확인\n",
    "        check_times = 6\n",
    "        for i in range(check_times):\n",
    "            await asyncio.sleep(5)  # 5초마다\n",
    "            status = scheduler.get_task_status(\"test_task_1\")\n",
    "            print(\n",
    "                f\"\\nTask Status at {datetime.now(pytz.utc)} (Check {i+1}/{check_times}):\"\n",
    "            )\n",
    "            print(json.dumps(status, indent=2))\n",
    "\n",
    "        # 정리\n",
    "        monitoring_task.cancel()\n",
    "        queue_processing_task.cancel()\n",
    "\n",
    "    # 테스트 실행\n",
    "    asyncio.get_event_loop().run_until_complete(test_scheduler())\n",
    "\n",
    "\n",
    "# 테스트 실행\n",
    "run_scheduler_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResponseAgent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: ResponseAgent 클래스 정의\n",
    "class ResponseAgent:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.3)\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        # 응답 생성을 위한 프롬프트\n",
    "        self.response_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            다음 RAG 논문 분석 결과를 바탕으로 사용자에게 전달할 응답을 생성해주세요.\n",
    "            \n",
    "            분석 결과:\n",
    "            {analysis_results}\n",
    "            \n",
    "            다음 형식으로 응답해주세요:\n",
    "            1. 핵심 요약 (2-3줄)\n",
    "            2. 추천 논문 목록 (관련도 순)\n",
    "            3. 각 논문별 주요 포인트\n",
    "            4. 실용적 시사점\n",
    "            \n",
    "            응답은 다음 JSON 형식으로 작성해주세요:\n",
    "            {{\n",
    "                \"summary\": \"전체 요약\",\n",
    "                \"recommended_papers\": [\n",
    "                    {{\"title\": \"제목\", \"relevance\": \"관련도\", \"key_points\": [\"포인트1\", \"포인트2\"]}}\n",
    "                ],\n",
    "                \"practical_implications\": [\"시사점1\", \"시사점2\"],\n",
    "                \"next_steps\": [\"추천 액션1\", \"추천 액션2\"]\n",
    "            }}\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 응답 생성 메서드\n",
    "def format_response(\n",
    "    self, rag_results: Dict[str, Any], query_info: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"RAG 결과를 기반으로 응답 생성\"\"\"\n",
    "    try:\n",
    "        # 분석 결과를 문자열로 변환\n",
    "        analysis_str = json.dumps(rag_results, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # LLM을 사용하여 응답 생성\n",
    "        response_chain = self.response_prompt | self.llm\n",
    "        response = response_chain.invoke({\"analysis_results\": analysis_str})\n",
    "\n",
    "        try:\n",
    "            formatted_response = json.loads(response.content)\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"response\": formatted_response,\n",
    "                \"original_query\": query_info[\"original_query\"],\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": \"Response formatting failed\",\n",
    "                \"raw_response\": response.content,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "ResponseAgent.format_response = format_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 메시지 생성 메서드\n",
    "def generate_message(self, formatted_response: Dict[str, Any]) -> str:\n",
    "    \"\"\"사용자에게 보낼 최종 메시지 생성\"\"\"\n",
    "    try:\n",
    "        message = []\n",
    "\n",
    "        # 전체 요약 추가\n",
    "        message.append(f\"📌 요약:\\n{formatted_response['summary']}\\n\")\n",
    "\n",
    "        # 추천 논문 목록\n",
    "        message.append(\"📚 추천 논문:\")\n",
    "        for paper in formatted_response[\"recommended_papers\"]:\n",
    "            message.append(f\"\\n제목: {paper['title']}\")\n",
    "            message.append(f\"관련도: {paper['relevance']}\")\n",
    "            message.append(\"주요 포인트:\")\n",
    "            for point in paper[\"key_points\"]:\n",
    "                message.append(f\"- {point}\")\n",
    "\n",
    "        # 실용적 시사점\n",
    "        message.append(\"\\n💡 실용적 시사점:\")\n",
    "        for implication in formatted_response[\"practical_implications\"]:\n",
    "            message.append(f\"- {implication}\")\n",
    "\n",
    "        # 다음 단계\n",
    "        message.append(\"\\n🔜 추천 액션:\")\n",
    "        for step in formatted_response[\"next_steps\"]:\n",
    "            message.append(f\"- {step}\")\n",
    "\n",
    "        return \"\\n\".join(message)\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"메시지 생성 중 오류 발생: {str(e)}\"\n",
    "\n",
    "\n",
    "ResponseAgent.generate_message = generate_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Response:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"response\": {\n",
      "    \"summary\": \"RAG 기술의 중요한 발전이 있었으며, 개선된 검색 방법, LLM과의 효율적인 통합, 향상된 성능 지표 등이 주요 개발 사항으로 소개되었습니다.\",\n",
      "    \"recommended_papers\": [\n",
      "      {\n",
      "        \"title\": \"Recent Advances in RAG\",\n",
      "        \"relevance\": \"높음\",\n",
      "        \"key_points\": [\n",
      "          \"Improved retrieval methods\",\n",
      "          \"Better integration with LLMs\",\n",
      "          \"Enhanced performance metrics\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"practical_implications\": [\n",
      "      \"RAG 기술의 발전을 따라가며 적용할 수 있는 방법을 고민해보는 것이 중요합니다.\"\n",
      "    ],\n",
      "    \"next_steps\": [\n",
      "      \"RAG 기술을 실제 업무나 연구에 적용해보고 성능을 평가해보는 것이 좋을 것입니다.\"\n",
      "    ]\n",
      "  },\n",
      "  \"original_query\": \"Find RAG related papers at 7 AM and recommend them\"\n",
      "}\n",
      "\n",
      "Final Message:\n",
      "📌 요약:\n",
      "RAG 기술의 중요한 발전이 있었으며, 개선된 검색 방법, LLM과의 효율적인 통합, 향상된 성능 지표 등이 주요 개발 사항으로 소개되었습니다.\n",
      "\n",
      "📚 추천 논문:\n",
      "\n",
      "제목: Recent Advances in RAG\n",
      "관련도: 높음\n",
      "주요 포인트:\n",
      "- Improved retrieval methods\n",
      "- Better integration with LLMs\n",
      "- Enhanced performance metrics\n",
      "\n",
      "💡 실용적 시사점:\n",
      "- RAG 기술의 발전을 따라가며 적용할 수 있는 방법을 고민해보는 것이 중요합니다.\n",
      "\n",
      "🔜 추천 액션:\n",
      "- RAG 기술을 실제 업무나 연구에 적용해보고 성능을 평가해보는 것이 좋을 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: 테스트 코드\n",
    "def test_response_agent():\n",
    "    # RAG 결과 시뮬레이션\n",
    "    rag_results = {\n",
    "        \"summaries\": [\n",
    "            {\n",
    "                \"title\": \"Recent Advances in RAG\",\n",
    "                \"summary\": \"This paper discusses the latest developments in RAG...\",\n",
    "                \"key_points\": [\n",
    "                    \"Improved retrieval methods\",\n",
    "                    \"Better integration with LLMs\",\n",
    "                    \"Enhanced performance metrics\",\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"recommendations\": [\n",
    "            {\n",
    "                \"title\": \"Recent Advances in RAG\",\n",
    "                \"reason\": \"Comprehensive overview of latest developments\",\n",
    "            }\n",
    "        ],\n",
    "        \"overall_analysis\": \"Significant progress in RAG technologies...\",\n",
    "    }\n",
    "\n",
    "    query_info = {\n",
    "        \"original_query\": \"Find RAG related papers at 7 AM and recommend them\"\n",
    "    }\n",
    "\n",
    "    agent = ResponseAgent()\n",
    "\n",
    "    # 응답 포맷팅\n",
    "    formatted_response = agent.format_response(rag_results, query_info)\n",
    "    print(\"\\nFormatted Response:\")\n",
    "    print(json.dumps(formatted_response, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # 최종 메시지 생성\n",
    "    if formatted_response[\"status\"] == \"success\":\n",
    "        message = agent.generate_message(formatted_response[\"response\"])\n",
    "        print(\"\\nFinal Message:\")\n",
    "        print(message)\n",
    "\n",
    "\n",
    "# 테스트 실행\n",
    "test_response_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: VectorDB 클래스 정의\n",
    "class VectorDB:\n",
    "    def __init__(self, persist_directory=\"./vector_db\"):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.persist_directory = persist_directory\n",
    "        self.ensure_directory()\n",
    "        self.db = self.initialize_db()\n",
    "\n",
    "    def ensure_directory(self):\n",
    "        \"\"\"벡터 DB를 저장할 디렉토리 생성\"\"\"\n",
    "        if not os.path.exists(self.persist_directory):\n",
    "            os.makedirs(self.persist_directory)\n",
    "            print(f\"Created directory: {self.persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 데이터베이스 초기화 및 관리 메서드\n",
    "def initialize_db(self) -> Chroma:\n",
    "    \"\"\"ChromaDB 초기화\"\"\"\n",
    "    return Chroma(\n",
    "        persist_directory=self.persist_directory, embedding_function=self.embeddings\n",
    "    )\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "VectorDB.initialize_db = initialize_db\n",
    "\n",
    "\n",
    "def store_papers(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"논문 정보 저장\"\"\"\n",
    "    try:\n",
    "        # 논문 컨텐츠와 메타데이터 준비\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "        for paper in papers:\n",
    "            texts.append(paper[\"content\"])\n",
    "            metadatas.append(\n",
    "                {\n",
    "                    \"title\": paper[\"title\"],\n",
    "                    \"source\": paper.get(\"source\", \"unknown\"),\n",
    "                    \"url\": paper.get(\"url\", \"\"),\n",
    "                    \"year\": paper.get(\"year\", \"\"),\n",
    "                    \"stored_at\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # 벡터 DB에 저장\n",
    "        self.db.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Stored {len(papers)} papers successfully\",\n",
    "            \"stored_count\": len(papers),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: store_papers 메서드 추가\n",
    "def store_papers(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"논문 정보 저장\"\"\"\n",
    "    try:\n",
    "        # 논문 컨텐츠와 메타데이터 준비\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "        for paper in papers:\n",
    "            texts.append(paper[\"content\"])\n",
    "            metadatas.append(\n",
    "                {\n",
    "                    \"title\": paper[\"title\"],\n",
    "                    \"source\": paper.get(\"source\", \"unknown\"),\n",
    "                    \"url\": paper.get(\"url\", \"\"),\n",
    "                    \"year\": paper.get(\"year\", \"\"),\n",
    "                    \"stored_at\": datetime.now().isoformat(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # 벡터 DB에 저장\n",
    "        self.db.add_texts(texts=texts, metadatas=metadatas)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Stored {len(papers)} papers successfully\",\n",
    "            \"stored_count\": len(papers),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "VectorDB.store_papers = store_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 검색 메서드 추가\n",
    "def search_papers(self, query: str, n_results: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"유사도 기반 논문 검색\"\"\"\n",
    "    try:\n",
    "        results = self.db.similarity_search_with_relevance_scores(query, k=n_results)\n",
    "\n",
    "        formatted_results = []\n",
    "        for doc, score in results:\n",
    "            formatted_results.append(\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"relevance_score\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\"status\": \"success\", \"results\": formatted_results, \"query\": query}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"query\": query}\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "VectorDB.search_papers = search_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 메타데이터 검색 및 관리 메서드 추가\n",
    "def get_paper_by_metadata(self, metadata_field: str, value: str) -> Dict[str, Any]:\n",
    "    \"\"\"메타데이터 기반 논문 검색\"\"\"\n",
    "    try:\n",
    "        results = self.db.get(where={metadata_field: value})\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"results\": results,\n",
    "            \"search_criteria\": {metadata_field: value},\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"search_criteria\": {metadata_field: value},\n",
    "        }\n",
    "\n",
    "\n",
    "def get_collection_stats(self) -> Dict[str, Any]:\n",
    "    \"\"\"컬렉션 통계 정보 조회\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"stats\": {\n",
    "                \"total_documents\": len(self.db.get()[\"ids\"]),\n",
    "                \"persist_directory\": self.persist_directory,\n",
    "            },\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "def clear_database(self) -> Dict[str, Any]:\n",
    "    \"\"\"데이터베이스 초기화\"\"\"\n",
    "    try:\n",
    "        self.db = self.initialize_db()\n",
    "        return {\"status\": \"success\", \"message\": \"Database cleared successfully\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# 클래스에 메서드들 추가\n",
    "VectorDB.get_paper_by_metadata = get_paper_by_metadata\n",
    "VectorDB.get_collection_stats = get_collection_stats\n",
    "VectorDB.clear_database = clear_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jw/xf5nckzn6433mtsy89wpm4dr0000gn/T/ipykernel_25404/1474403248.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Store Result:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"message\": \"Stored 2 papers successfully\",\n",
      "  \"stored_count\": 2\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"content\": \"A comprehensive study on improving RAG performance in various scenarios...\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"conference\",\n",
      "        \"stored_at\": \"2025-01-22T00:21:22.263140\",\n",
      "        \"title\": \"Improving RAG Performance\",\n",
      "        \"url\": \"https://example.com/paper2\",\n",
      "        \"year\": \"2023\"\n",
      "      },\n",
      "      \"relevance_score\": 0.8446409962394603\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"This paper discusses recent advances in Retrieval Augmented Generation...\",\n",
      "      \"metadata\": {\n",
      "        \"source\": \"arxiv\",\n",
      "        \"stored_at\": \"2025-01-22T00:21:22.263134\",\n",
      "        \"title\": \"Recent Advances in RAG\",\n",
      "        \"url\": \"https://example.com/paper1\",\n",
      "        \"year\": \"2024\"\n",
      "      },\n",
      "      \"relevance_score\": 0.7008961117884753\n",
      "    }\n",
      "  ],\n",
      "  \"query\": \"RAG implementation techniques\"\n",
      "}\n",
      "\n",
      "Collection Stats:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"stats\": {\n",
      "    \"total_documents\": 2,\n",
      "    \"persist_directory\": \"./test_vector_db\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 테스트\n",
    "def test_vector_db():\n",
    "    # 테스트용 논문 데이터\n",
    "    test_papers = [\n",
    "        {\n",
    "            \"title\": \"Recent Advances in RAG\",\n",
    "            \"content\": \"This paper discusses recent advances in Retrieval Augmented Generation...\",\n",
    "            \"source\": \"arxiv\",\n",
    "            \"url\": \"https://example.com/paper1\",\n",
    "            \"year\": \"2024\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Improving RAG Performance\",\n",
    "            \"content\": \"A comprehensive study on improving RAG performance in various scenarios...\",\n",
    "            \"source\": \"conference\",\n",
    "            \"url\": \"https://example.com/paper2\",\n",
    "            \"year\": \"2023\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # VectorDB 인스턴스 생성\n",
    "    db = VectorDB(persist_directory=\"./test_vector_db\")\n",
    "\n",
    "    # 논문 저장\n",
    "    store_result = db.store_papers(test_papers)\n",
    "    print(\"\\nStore Result:\")\n",
    "    print(json.dumps(store_result, indent=2))\n",
    "\n",
    "    # 검색 테스트\n",
    "    search_result = db.search_papers(\"RAG implementation techniques\")\n",
    "    print(\"\\nSearch Result:\")\n",
    "    print(json.dumps(search_result, indent=2))\n",
    "\n",
    "    # 통계 정보 확인\n",
    "    stats = db.get_collection_stats()\n",
    "    print(\"\\nCollection Stats:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "\n",
    "\n",
    "# 테스트 실행\n",
    "test_vector_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
