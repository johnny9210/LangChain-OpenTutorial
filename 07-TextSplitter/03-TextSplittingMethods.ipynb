{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Text Splitting Methods\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Peer Review : [JoonHo Kim](https://github.com/jhboyo), [Sunyoung Park (architectyou)](https://github.com/Architectyou)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/03-TokenTextSplitter.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/03-TokenTextSplitter.ipynb)\n",
    "\n",
    "## Overview\n",
    "Text splitting is a crucial preprocessing step in Natural Language Processing (NLP). This tutorial covers various text splitting methods and tools, exploring their advantages, disadvantages, and appropriate use cases.\n",
    "\n",
    "Main approaches to text splitting:\n",
    "\n",
    "1. **Token-based Splitting**\n",
    "   - Tiktoken: OpenAI's high-performance BPE tokenizer\n",
    "   - Hugging Face tokenizers: Tokenizers for various pre-trained models\n",
    "   \n",
    "2. **Sentence-based Splitting**\n",
    "   - SentenceTransformers: Splits text while maintaining semantic coherence\n",
    "   - NLTK: Natural language processing based sentence and word splitting\n",
    "   - spaCy: Text splitting utilizing advanced language processing capabilities\n",
    "\n",
    "3. **Language-specific Tools**\n",
    "   - KoNLPy: Specialized splitting tool for Korean text processing\n",
    "\n",
    "Each tool has its unique characteristics and advantages:\n",
    "- Tiktoken offers fast processing speed and compatibility with OpenAI models\n",
    "- SentenceTransformers provides meaning-based sentence splitting\n",
    "- NLTK and spaCy implement linguistic rule-based splitting\n",
    "- KoNLPy specializes in Korean morphological analysis and splitting\n",
    "\n",
    "Through this tutorial, you will understand the characteristics of each tool and learn to choose the most suitable text splitting method for your project.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Example Usage of Tiktoken](#example-usage-of-tiktoken)\n",
    "- [Example Usage of TokenTextSplitter](#example-usage-of-tokentextsplitter)\n",
    "- [Example Usage of SentenceTransformers](#example-usage-of-sentencetransformers)\n",
    "- [Example Usage of NLTK](#example-usage-of-nltk)\n",
    "- [Example Usage of spaCy](#example-usage-of-spacy)\n",
    "- [Example Usage of KoNLPy](#example-usage-of-konlpy)\n",
    "- [Example Usage of Hugging Face tokenizers](#example-usage-of-hugging-face-tokenizers)\n",
    "\n",
    "### References\n",
    "- [Langchain: Tiktoken](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)\n",
    "- [Langchain: TokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html)\n",
    "- [Langchain: SentenceTransformers](https://python.langchain.com/api_reference/text_splitters/sentence_transformers/langchain_text_splitters.sentence_transformers.SentenceTransformersTokenTextSplitter.html)\n",
    "- [Langchain: NLTK](https://python.langchain.com/api_reference/text_splitters/nltk/langchain_text_splitters.nltk.NLTKTextSplitter.html)\n",
    "- [LangChain: spaCy](https://python.langchain.com/api_reference/text_splitters/spacy/langchain_text_splitters.spacy.SpacyTextSplitter.html)\n",
    "- [LangChain: KoNLPy](https://python.langchain.com/api_reference/text_splitters/konlpy/langchain_text_splitters.konlpy.KonlpyTextSplitter.html)\n",
    "- [LangChain: Hugging Face tokenizers](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)\n",
    "- [LangChain: How to split text by tokens](https://python.langchain.com/docs/how_to/split_by_token/)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
    "\n",
    "**[Note]**\n",
    "- The `langchain-opentutorial` is a bundle of easy-to-use environment setup guidance, useful functions and utilities for tutorials. \n",
    "- Check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"tiktoken\",\n",
    "        \"spacy\",\n",
    "        \"sentence-transformers\",\n",
    "        \"nltk\",\n",
    "        \"konlpy\",\n",
    "    ],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"TokenTextSplitter\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can set and load `OPENAI_API_KEY` from a `.env` file. \n",
    "\n",
    "**[Note]** This is only necessary if you haven't already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tiktoken for Text Splitting\n",
    "\n",
    "`tiktoken` is a fast BPE (Byte Pair Encoding) tokenizer developed by OpenAI. Here's an example demonstrating its use with a text splitter:\n",
    "\n",
    "1. Open the text file `appendix-keywords.txt` and read its contents. Store this text in a variable named `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display some of the content read from the `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to unders\n"
     ]
    }
   ],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `CharacterTextSplitter` with `tiktoken`:\n",
    "\n",
    "1. Initialize a text splitter using the `from_tiktoken_encoder` method. This method leverages the `tiktoken` encoder for measurement and merging.\n",
    "\n",
    "   * `chunk_size=300`: This parameter determines the maximum size of each text chunk in tokens. The splitter will attempt to create chunks that contain no more than 300 tokens each. This number can be adjusted based on your specific needs and model token limits. This parameter will be used consistently throughout the tutorial with the same meaning.\n",
    "\n",
    "   * `chunk_overlap=50`: This defines how many tokens should overlap between consecutive chunks. Overlap helps maintain context and coherence between chunks, especially useful when the text contains concepts that span chunk boundaries. A value of 50 means each chunk will share its last 50 tokens with the beginning of the next chunk. This parameter will appear multiple times in this tutorial, always serving the same purpose.\n",
    "\n",
    "Note: Throughout this tutorial, whenever you see `chunk_size` and `chunk_overlap` parameters in different text splitters, they follow the same principles as described above, though the exact implementation might vary depending on the specific splitter being used.\n",
    "\n",
    "This combination of character-based splitting with tiktoken-based measurement provides a good balance between processing speed and accurate chunk sizing, particularly when working with models that use tiktoken for tokenization (like OpenAI's models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # Set the chunk size to 300.\n",
    "    chunk_size=300,\n",
    "    # Set the overlap between chunks to 50.\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the number of resulting text chunks after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))  # Output the number of divided chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print the first element of the `texts` list, which holds the split chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to understand and process the text.\n",
      "Example: Represent the word “apple” as a vector such as [0.65, -0.23, 0.17].\n",
      "Related keywords: natural language processing, vectorization, deep learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token is a breakup of text into smaller units. These can typically be words, sentences, or phrases.\n",
      "Example: Split the sentence “I am going to school” into “I am”, “to school”, and “going”.\n",
      "Associated keywords: tokenization, natural language processing, parsing\n",
      "\n",
      "Tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Print the first element of the texts list.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- When using `CharacterTextSplitter.from_tiktoken_encoder`, the text is split primarily by the `CharacterTextSplitter`. The `tiktoken` tokenizer is used for measuring and merging the divided text. This might lead to chunks exceeding the token size intended for the language model.\n",
    "- Consider `RecursiveCharacterTextSplitter.from_tiktoken_encoder` or directly loading the `tiktoken` splitter, for stricter control and ensuring each split adheres to the language model's token limit. If a split text exceeds this size, it is recursively divided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of TokenTextSplitter\n",
    "This section will cover using the `TokenTextSplitter` class to split text into chunks based on tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to understand and process the text.\n",
      "Example: Represent the word “apple” as a vector such as [0.65, -0.23, 0.17].\n",
      "Related keywords: natural language processing, vectorization, deep learning\n",
      "\n",
      "Token\n",
      "\n",
      "Definition: A token is a breakup of text into smaller units. These can typically be words, sentences, or phrases.\n",
      "Example: Split the sentence “I am going to school” into “I am”, “to school”, and “going”.\n",
      "Associated keywords: tokenization, natural language processing, parsing\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is used to preprocess data in natural language processing.\n",
      "Example: Split the sentence “I love programming.” into [“I”, “love”, “program\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=300,  # Set the chunk size to 300.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")\n",
    "\n",
    "# Split the state_of_the_union text into chunks.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first chunk of the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of SentenceTransformers\n",
    "\n",
    "`SentenceTransformersTokenTextSplitter` is a specialized splitter designed for `sentence-transformer` models. It automatically splits text into chunks that fit within the token window of the sentence-transformer model being used.\n",
    "\n",
    "1. Initialize a text splitter using the `SentenceTransformersTokenTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create a sentence splitter and set the overlap between chunks to 50.\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the number of tokens (excluding start and stop tokens) in the `file` variable, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n"
     ]
    }
   ],
   "source": [
    "count_start_and_stop_tokens = 2  # Set the number of start and stop tokens to 2.\n",
    "\n",
    "# Subtract the count of start and stop tokens from the total number of tokens in the text.\n",
    "text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # Print the calculated number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Utilize the `splitter.split_text()` function to split the text stored in `text_to_split` into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=file)  # Split the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print the first chunk using `print(text_chunks[1])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a database for quick access. related keywords : embedding, database, vectorization, vectorization sql definition : sql ( structured query language ) is a programming language for managing data in a database. you can query, modify, insert, delete, and more data. example : select * from users where age > 18 ; looks up information about users who are 18 years old or older. associated keywords : database, query, data management, data management csv definition : csv ( comma - separated values ) is a file format for storing data, where each data value is separated by a comma. it is used for simple storage and exchange of tabular data. example : a csv file with the headers name, age, and occupation might contain data such as hong gil - dong, 30, developer. related keywords : data format, file processing, data exchange json definition : json ( javascript object notation ) is a lightweight data interchange format that represents data objects using text that is readable to both humans and machines. example : { “ name ” : “ honggildong ”, ‘ age ’ : 30, “ occupation ” : “ developer \" } is data in json format. related keywords : data exchange, web development, apis transformer definition : transformers are a type of deep learning model used in natural language processing, mainly for translation, summarization, text generation, etc. it is based on the attention mechanism. example : google translator uses transformer models to perform translations between different languages. related keywords : deep learning, natural language processing, attention huggingface definition : huggingface is a library that provides a variety of pre - trained models and tools for natural language processing. it helps researchers and developers to easily perform nlp tasks. example : you can use huggingface's transformers library to perform tasks such as sentiment analysis, text generation\n"
     ]
    }
   ],
   "source": [
    "# Print the 0th chunk.\n",
    "print(text_chunks[1])  # Print the second chunk from the divided text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a Python library for natural language processing (NLP) tasks. It supports various NLP tasks like text preprocessing, tokenization, morphological analysis, and part-of-speech tagging.\n",
    "\n",
    "Here's how to use NLTK tokenizers for text splitting, offering an alternative to splitting by newlines (`\\n\\n`).\n",
    "- Splitting method: NLTK tokenizer\n",
    "- The chunk size is determined by the number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Before using NLTK, you need to download the necessary data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ilgyun/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading `punkt_tab` enables NLTK to tokenize text into words or sentences for multiple languages, including English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a text splitter using the `NLTKTextSplitter` class.\n",
    "4. Set the `chunk_size` parameter to 300 (or any desired value) to control the mazimum chunk size in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=300,  # Set the chunk size to 300.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Utilize the `split_text` method of the `text_splitter` object to split the text stored in `file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format.\n",
      "\n",
      "It is used for search, classification, and other data analysis tasks.\n",
      "\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of spaCy\n",
    "\n",
    "spaCy is an open-source library for advanced NLP, written in Python and Cython.\n",
    "\n",
    "Like NLTK, spaCy also provides an alternative to basic newline splitting (`\\n\\n`).\n",
    "- Splitting method: spaCy's tokenizer\n",
    "- The chunk size is measured by the number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To split text with spaCy, you need to download the `en_core_web_sm` spaCy model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the file data/appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "\n",
    "\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a text splitter using the `SpacyTextSplitter` class.\n",
    "4. Set the `chunk_size` parameter to 300 (or any desired value) to control the mazimum chunk size in characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# Ignore  warning messages.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create the SpacyTextSplitter.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=300,  # Set the chunk size to 300.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the `split_text` method of the `text_splitter` object to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format.\n",
      "\n",
      "It is used for search, classification, and other data analysis tasks.\n",
      "\n",
      "\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n"
     ]
    }
   ],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of KoNLPy\n",
    "\n",
    "As mentioned in the [LangChain's How-to guides](https://python.langchain.com/docs/how_to/split_by_token/#konlpy), KoNLPy offers a dedicated text splitter for Korean text processing with useful features for morphological analysis, part-of-speech tagging, and syntactic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since it is an example of processing Korean language, we need to log Korean text to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords-korean.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords-korean.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a text splitter using the `KonlpyTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# Create a text splitter object using KonlpyTextSplitter.\n",
    "text_splitter = KonlpyTextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the `text_splitter` to split `the file` content into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search 정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "예시: 사용자가 \" 태양계 행성\" 이라고 검색하면, \" 목성\", \" 화 성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝 Embedding 정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다.\n",
      "\n",
      "이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "\n",
      "예시: \" 사과\" 라는 단어를 [0.65, -0.23, 0.17] 과 같은 벡터로 표현합니다.\n",
      "\n",
      "연\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)  # Split the file content into sentences.\n",
    "print(texts[0][:350])  # Print the first sentence from the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of Hugging Face tokenizers\n",
    "\n",
    "Hugging Face provides various tokenizers.\n",
    "\n",
    "This tutorial demonstrates calculating the token length of a text using one of Hugging Face's tokenizers, `GPT2TokenizerFast`.\n",
    "- Splitting method: Hugging Face's `GPT2TokenizerFast`\n",
    "- The chunk size is determined by the number of characters\n",
    "\n",
    "**[Note]**\n",
    "- The chunk size is based on the number of tokens calculated by the Hugging Face tokenizer.\n",
    "- A `tokenizer` object is created using the `GPT2TokenizerFast` class.\n",
    "\n",
    "Call `from_pretrained` method to load the pre-trained `gpt2` tokenizer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Call `from_pretrained` method to load the pre-trained `gpt2` tokenizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load the GPT-2 tokenizer.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeate the process of opening `appendix-keywords.txt`, reading its contents, and storing the text in the `file` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embed\n"
     ]
    }
   ],
   "source": [
    "# Open the data/appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a text splitter using `from_huggingface_tokenizer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    # Use the Hugging Face tokenizer to create a CharacterTextSplitter object.\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Check the split result of the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is used to preprocess data in natural language processing.\n",
      "Example: Split the sentence “I love programming.” into [“I”, “love”, “programming”, “.”].\n",
      "Associated keywords: tokenization, natural language processing, parsing\n",
      "\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "SQL\n",
      "\n",
      "Definition: SQL(Structured Query Language) is a programming language for managing data in a database. You can query, modify, insert, delete, and more data.\n",
      "Example: SELECT * FROM users WHERE age > 18; looks up information about users who are 18 years old or older.\n",
      "Associated keywords: database, query, data management, data management\n",
      "\n",
      "CSV\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])  # Print the first element of the texts list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
